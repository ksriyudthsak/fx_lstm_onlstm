{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert jupyter to python\n",
    "# !jupyter-nbconvert --to script gan_mlp_lstm_fx.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# make only specific GPU to be utilized\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "\n",
    "# select GPU to run on\n",
    "GPU = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU\n",
    "\n",
    "# stop GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"-1\"\n",
    "\n",
    "# set GPU to be deterministic \n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\" # set hash environment\n",
    "os.environ[\"TF_CUDNN_USE_AUTOTUNE\"] = \"0\" # use cuDNN function to retrieve the best algorithm\n",
    "os.environ[\"TF_CUDNN_CONVOLUTION_BWD_FILTER_ALGO_DETERMINISTIC\"]='1' # use cuDNN deterministic algorithms\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1' # deterministic setting\n",
    "\n",
    "# set numpy, python, tensorflow random seed\n",
    "from numpy.random import seed\n",
    "import random\n",
    "random.seed(10)\n",
    "seed(10)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(20)\n",
    "\n",
    "# control GPU memory usages\n",
    "import tensorflow as tf\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.25\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0921 04:42:46.798807 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0921 04:42:46.801268 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0921 04:42:46.803543 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0921 04:42:46.809345 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available GPU\n",
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1636503586888763817\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7445285003381921542\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2630255341544950789\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4228513792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12783924363352060845\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:05.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# check devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "plt.rcParams[\"font.size\"] = 18\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from keras.layers import Dense, Dropout, Input, Reshape, Bidirectional, Embedding\n",
    "from keras.layers import LSTM, Conv1D, BatchNormalization, Activation, MaxPooling1D, Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam, SGD, Adamax, RMSprop\n",
    "from keras_ordered_neurons import ONLSTM\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.callbacks import CSVLogger, TensorBoard\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data acquisition and pre-processing, see data_process_fx.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !ls "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>112.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-03 10:00:00</td>\n",
       "      <td>112.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-04 10:00:00</td>\n",
       "      <td>112.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-05 10:00:00</td>\n",
       "      <td>113.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-08 10:00:00</td>\n",
       "      <td>113.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime    Open\n",
       "0  2018-01-02 10:00:00  112.37\n",
       "1  2018-01-03 10:00:00  112.33\n",
       "2  2018-01-04 10:00:00  112.61\n",
       "3  2018-01-05 10:00:00  113.19\n",
       "4  2018-01-08 10:00:00  113.22"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get train dataset\n",
    "df_train = pd.read_csv(\"USDJPY/usdjpy_train.csv\")\n",
    "# df_train = pd.read_csv(\"USDJPY/usdjpy_train_2010-2017.csv\")\n",
    "# df_train = pd.read_csv(\"USDJPY/usdjpy_train_2015-2017.csv\")\n",
    "\n",
    "if \"Date\" in df_train.columns:\n",
    "    df_train.sort_values(\"Date\", inplace=True)\n",
    "else:\n",
    "    df_train.sort_values(\"datetime\", inplace=True)   \n",
    "df_train.head()\n",
    "# df_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019-01-01 10:00:00</td>\n",
       "      <td>109.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-02 10:00:00</td>\n",
       "      <td>108.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-01-03 10:00:00</td>\n",
       "      <td>107.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019-01-04 10:00:00</td>\n",
       "      <td>108.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019-01-07 10:00:00</td>\n",
       "      <td>108.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime    Open\n",
       "0  2019-01-01 10:00:00  109.59\n",
       "1  2019-01-02 10:00:00  108.94\n",
       "2  2019-01-03 10:00:00  107.63\n",
       "3  2019-01-04 10:00:00  108.01\n",
       "4  2019-01-07 10:00:00  108.19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get test dataset\n",
    "df_test0 = pd.read_csv(\"USDJPY/usdjpy_test.csv\")\n",
    "# df_test0 = pd.read_csv(\"USDJPY/usdjpy_test_2018-2019.csv\")\n",
    "# df_test0 = pd.read_csv(\"USDJPY/usdjpy_test_2018-2019.csv\")\n",
    "\n",
    "if \"Date\" in df_train.columns:\n",
    "    df_test0.sort_values(\"Date\", inplace=True)\n",
    "else:\n",
    "    df_test0.sort_values(\"datetime\", inplace=True)\n",
    "df_test0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert price data to diff price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://philipperemy.github.io/keras-stateful-lstm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>Open</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-02 10:00:00</td>\n",
       "      <td>112.37</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-03 10:00:00</td>\n",
       "      <td>112.33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-04 10:00:00</td>\n",
       "      <td>112.61</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-05 10:00:00</td>\n",
       "      <td>113.19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-08 10:00:00</td>\n",
       "      <td>113.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime    Open  diff\n",
       "0  2018-01-02 10:00:00  112.37     0\n",
       "1  2018-01-03 10:00:00  112.33     0\n",
       "2  2018-01-04 10:00:00  112.61     1\n",
       "3  2018-01-05 10:00:00  113.19     1\n",
       "4  2018-01-08 10:00:00  113.22     1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_train[\"diff\"] = np.where(df_train[\"Open\"].diff()>=0, 1, -1) \n",
    "# df_test0[\"diff\"] = np.where(df_test0[\"Open\"].diff()>=0, 1, -1) \n",
    "df_train[\"diff\"] = np.where(df_train[\"Open\"].diff()>0, 1, 0) \n",
    "df_test0[\"diff\"] = np.where(df_test0[\"Open\"].diff()>0, 1, 0) \n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up parameter and process data shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up parameters and features for training\n",
    "batch_size = 60 #60\n",
    "lstm_batch_size = 60 #60 #timestep\n",
    "epochs = 200 #100\n",
    "split_validate_test = 0.5\n",
    "selected_features = [\"Open\"] # [\"Open\", \"Vol\"] # the first column should be the one would like to predict\n",
    "# selected_features = [\"diff\"] # [\"Open\", \"Vol\"] # the first column should be the one would like to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_train (259, 1)\n",
      "benchmark_actual_train (199, 1)\n",
      "num_features 1\n",
      "features (199, 60, 1)\n",
      "labels (199,)\n"
     ]
    }
   ],
   "source": [
    "# process train data\n",
    "data_train = df_train[selected_features].values \n",
    "print (\"data_train\", data_train.shape)\n",
    "benchmark_actual_train = df_train[selected_features].iloc[lstm_batch_size:].values\n",
    "print (\"benchmark_actual_train\", benchmark_actual_train.shape)\n",
    "\n",
    "# calculate number of features\n",
    "num_features = data_train.shape[1]\n",
    "print (\"num_features\", num_features)\n",
    "\n",
    "# normalise data\n",
    "scaler_train = MinMaxScaler(feature_range = (0, 1))\n",
    "data_train_scaled = scaler_train.fit_transform(data_train)\n",
    "scaler_train_output = MinMaxScaler(feature_range = (0, 1))\n",
    "data_train_scaled_output = scaler_train_output.fit_transform(data_train[:,0:1])\n",
    "\n",
    "# convert data shape for lstm\n",
    "def convert_data_shape_for_lstm(data_scaled, lstm_batch_size):\n",
    "    features0 = []\n",
    "    labels0 = []\n",
    "    for i in range(lstm_batch_size, len(data_scaled)):\n",
    "        features0.append(data_scaled[i-lstm_batch_size:i, 0:data_scaled.shape[1]])\n",
    "        labels0.append(data_scaled[i, 0])\n",
    "    # reshape\n",
    "    features0, labels0 = np.array(features0), np.array(labels0)\n",
    "    features0 = np.reshape(features0, (features0.shape[0], features0.shape[1], data_scaled.shape[1]))\n",
    "    print (\"features\", features0.shape)\n",
    "    print (\"labels\", labels0.shape)\n",
    "    return features0, labels0\n",
    "\n",
    "train_features, train_labels = convert_data_shape_for_lstm(data_train_scaled, lstm_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_total (411, 1)\n",
      "data_test (211, 1)\n",
      "features (151, 60, 1)\n",
      "labels (151,)\n",
      "test_features (76, 60, 1)\n",
      "validate_features (75, 60, 1)\n",
      "benchmark_actual_test (76, 1)\n"
     ]
    }
   ],
   "source": [
    "# process test data\n",
    "data_total = pd.concat((df_train[selected_features], df_test0[selected_features]), axis=0)\n",
    "print (\"data_total\", data_total.shape)\n",
    "data_test = data_total[len(data_total) - len(df_test0) - lstm_batch_size + 1:].values\n",
    "print (\"data_test\", data_test.shape)\n",
    "\n",
    "# normalise data\n",
    "scaler_test = MinMaxScaler(feature_range = (0, 1))\n",
    "data_test_scaled = scaler_train.fit_transform(data_test)\n",
    "\n",
    "# convert data shape for lstm\n",
    "test_features0, test_labels0 = convert_data_shape_for_lstm(data_test_scaled, lstm_batch_size)\n",
    "\n",
    "# seperate validate and test\n",
    "validate_features = test_features0[0:int(len(test_features0)*split_validate_test)]\n",
    "test_features = test_features0[int(len(test_features0)*split_validate_test):]\n",
    "validate_labels = test_labels0[0:int(len(test_labels0)*split_validate_test)]\n",
    "test_labels = test_labels0[int(len(test_labels0)*split_validate_test):]\n",
    "\n",
    "benchmark_actual_test = data_test[int((len(test_features0))*split_validate_test)+lstm_batch_size:,]\n",
    "\n",
    "print (\"test_features\", test_features.shape)\n",
    "print (\"validate_features\", validate_features.shape)\n",
    "print (\"benchmark_actual_test\", benchmark_actual_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_timeseries(epoch, training_type, discriminator_type, generator_type, generated_timeseries, real_data, examples=100):\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(generated_timeseries.T.shape[0]):\n",
    "        plt.rcParams[\"font.size\"] = 18\n",
    "        ax1 = plt.subplot(1, 1, i+1)\n",
    "        ax1.plot(generated_timeseries.T[0], color='r', label='predict', linewidth=3.0)\n",
    "        ax1.ticklabel_format(useOffset=False)\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(real_data, color='b', label=\"real\", linewidth=3.0)\n",
    "        ax2.ticklabel_format(useOffset=False)\n",
    "        plt.ylim([105,119])\n",
    "        ax1.legend(loc='upper left')\n",
    "        ax2.legend(loc='upper right')\n",
    "        plt.xlabel(\"Day\")\n",
    "        plt.ylabel(\"Price\")        \n",
    "        ax1.legend()\n",
    "        ax1.set_title('USDJPY: Epoch={}'.format(epoch), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/%s_%s_%s_generated_timeseries_%d.png' %(training_type, discriminator_type, generator_type, epoch))\n",
    "    #plt.close()\n",
    "    \n",
    "def plot_timeseries2(epoch, training_type, discriminator_type, generator_type, generated_timeseries, real_data, examples=100):\n",
    "    print (len(generated_timeseries.T[0]), len(real_data))\n",
    "    plt.figure(figsize=(8,5))\n",
    "    for i in range(generated_timeseries.T.shape[0]):\n",
    "        plt.rcParams[\"font.size\"] = 18\n",
    "        ax1 = plt.subplot(1, 1, i+1)\n",
    "        ax1.plot(generated_timeseries.T[0], color='r', label='predict', linewidth=3.0)\n",
    "        ax1.ticklabel_format(useOffset=False)\n",
    "        ax1.plot(real_data, color='b', label=\"real\", linewidth=3.0)\n",
    "        ax1.ticklabel_format(useOffset=False)\n",
    "        plt.xlabel(\"Day\")\n",
    "        plt.ylabel(\"Price (USDJPY)\")        \n",
    "        ax1.legend()\n",
    "        ax1.set_title('{}_{}_{}'.format(training_type, discriminator_type, generator_type), fontsize=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('output/%s_%s_%s_generated_timeseries_%d.png' %(training_type, discriminator_type, generator_type, epoch))\n",
    "    #plt.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_and_plot(case, model_case, selected_feature, features, model, benchmark_actual):\n",
    "    lstm_predict = model.predict(features)\n",
    "    print (lstm_predict.shape, features.shape)\n",
    "    if selected_feature == \"Open\":\n",
    "        lstm_predict = scaler_train_output.inverse_transform(lstm_predict)\n",
    "    plot_timeseries(epochs, case, \"\", model_case, lstm_predict, benchmark_actual)\n",
    "    plot_timeseries2(epochs, case, \"\", model_case, lstm_predict, benchmark_actual)   \n",
    "    return lstm_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix(selected_feature, model_prediction):\n",
    "    print (\"selected_feature\", selected_feature)\n",
    "    if selected_feature == \"Open\":\n",
    "        y_true = np.where(np.diff(benchmark_actual_test.T[0])>0, 1, -1) \n",
    "        y_pred = np.where(np.diff(model_prediction[:,0])>0, 1, -1) \n",
    "    elif selected_feature == \"diff\":\n",
    "        y_true = np.where(benchmark_actual_test.T[0]>=0, 1, -1) \n",
    "        y_pred = np.where(model_prediction[:,0]>=0.5, 1, -1) \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print (cm)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackabuse.com/time-series-analysis-with-lstm-using-pythons-keras-library/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 04:42:48.176748 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0921 04:42:49.553457 140216956897088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0921 04:42:53.349709 140216956897088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 60, 500)           1004000   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 60, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60, 500)           2002000   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60, 500)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 500)               2002000   \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 501       \n",
      "=================================================================\n",
      "Total params: 7,010,501\n",
      "Trainable params: 7,010,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_lstm_model():\n",
    "    lstm_model = Sequential()\n",
    "    lstm_model.add(LSTM(units=500, return_sequences=True, \n",
    "                        input_shape=(train_features.shape[1], train_features.shape[2])))\n",
    "    lstm_model.add(Dropout(0.01))\n",
    "    \n",
    "    lstm_model.add(LSTM(units=500, return_sequences=True))\n",
    "    lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    lstm_model.add(LSTM(units=500, return_sequences=True))\n",
    "    lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    lstm_model.add(LSTM(units=500))\n",
    "    lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    lstm_model.add(Dense(units = 1))\n",
    "\n",
    "    lstm_model.compile(optimizer = 'RMSprop', \n",
    "                       loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    return lstm_model\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199, 60, 1) (199,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0921 04:42:53.746042 140216956897088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 199 samples, validate on 75 samples\n",
      "Epoch 1/200\n"
     ]
    }
   ],
   "source": [
    "print (train_features.shape, train_labels.shape)\n",
    "lstm_history = lstm_model.fit(train_features, train_labels, epochs = epochs, batch_size = batch_size,\n",
    "                              validation_data=(validate_features, validate_labels),\n",
    "                              shuffle=False)\n",
    "lstm_score, lstm_acc = lstm_model.evaluate(test_features, test_labels, batch_size=batch_size, verbose=0)\n",
    "print (lstm_score, lstm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(lstm_history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lstm_prediction = predict_model_and_plot(\"train\", \"lstm\", selected_features[0], train_features, lstm_model, benchmark_actual_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predictions = predict_model_and_plot(\"test\", \"lstm\", selected_features[0], test_features, lstm_model, benchmark_actual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confusion_matrix(selected_features[0], lstm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordered-neurons LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/abs/1810.09536\n",
    "# https://github.com/CyberZHG/keras-ordered-neurons/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_on_lstm_model():\n",
    "    on_lstm_model = Sequential()\n",
    "    on_lstm_model.add(ONLSTM(units=500, chunk_size=5, return_sequences=True, \n",
    "                                        input_shape=(train_features.shape[1], train_features.shape[2])))\n",
    "    on_lstm_model.add(Dropout(0.01))\n",
    "    \n",
    "    on_lstm_model.add(ONLSTM(units=500, chunk_size=5, return_sequences=True))\n",
    "    on_lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    on_lstm_model.add(ONLSTM(units=500, chunk_size=5, return_sequences=True))\n",
    "    on_lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    on_lstm_model.add(ONLSTM(units=500, chunk_size=5))\n",
    "    on_lstm_model.add(Dropout(0.01))\n",
    "\n",
    "    on_lstm_model.add(Dense(units = 1))\n",
    "\n",
    "    on_lstm_model.compile(optimizer = 'RMSprop', \n",
    "                       loss = 'mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    return on_lstm_model\n",
    "on_lstm_model = create_on_lstm_model()\n",
    "on_lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on_lstm_history = on_lstm_model.fit(train_features, train_labels, epochs = epochs, batch_size = batch_size)\n",
    "on_lstm_history = on_lstm_model.fit(train_features, train_labels, \n",
    "                                    epochs = epochs, batch_size = batch_size,\n",
    "                                    validation_data=(validate_features, validate_labels),\n",
    "                                    shuffle=False)\n",
    "on_lstm_score, on_lstm_acc = on_lstm_model.evaluate(test_features, test_labels, \n",
    "                                                    batch_size=batch_size, verbose=0)\n",
    "print (on_lstm_score, on_lstm_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "plt.plot(on_lstm_history.history['loss'], label='train')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_lstm_predict = predict_model_and_plot(\"train\", \"onlstm\", selected_features[0], train_features, on_lstm_model, benchmark_actual_train)\n",
    "# train_on_lstm_predict = on_lstm_model.predict(train_features)\n",
    "# print (train_on_lstm_predict.shape, train_features.shape)\n",
    "# train_on_lstm_predict = scaler_train_output.inverse_transform(train_on_lstm_predict)\n",
    "# plot_timeseries(0, \"train\", \"\", \"onlstm\", train_on_lstm_predict, benchmark_actual_train)\n",
    "# plot_timeseries2(0, \"train\", \"\", \"onlstm\", train_on_lstm_predict, benchmark_actual_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_lstm_predictions = predict_model_and_plot(\"test\", \"onlstm\", selected_features[0], test_features, on_lstm_model, benchmark_actual_test)\n",
    "# on_lstm_predictions = on_lstm_model.predict(test_features)\n",
    "# plot_timeseries2(0, \"test\", \"\", \"onlstm\", on_lstm_predictions, \n",
    "#                  data_test_scaled[int(lstm_batch_size+len(test_features0)*split_validate_test):])\n",
    "# on_lstm_predictions = scaler_train_output.inverse_transform(on_lstm_predictions)\n",
    "# plot_timeseries(0, \"test\", \"\", \"onlstm\", on_lstm_predictions, benchmark_actual_test)\n",
    "# plot_timeseries2(0, \"test\", \"\", \"onlstm\", on_lstm_predictions, benchmark_actual_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confusion_matrix(selected_features[0], on_lstm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://medium.com/datadriveninvestor/generative-adversarial-network-gan-using-keras-ce1c05cfdfd3\n",
    "# https://qiita.com/taku-buntu/items/0093a68bfae0b0ff879d\n",
    "# https://qiita.com/yoyoyo_/items/56c6fcbd5a853460f506"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    discriminator=Sequential()\n",
    "    discriminator.add(Dense(units=512,input_dim=1))\n",
    "    discriminator.add(LeakyReLU(alpha=0.1))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(alpha=0.1))\n",
    "    discriminator.add(Dense(128))\n",
    "    discriminator.add(LeakyReLU(alpha=0.1))\n",
    "    discriminator.add(Dense(units=1, activation='sigmoid'))    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer='RMSprop')\n",
    "    return discriminator\n",
    "d =create_discriminator()\n",
    "d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_gan(discriminator, generator, generator_type=\"lstm\"):\n",
    "    discriminator.trainable=False\n",
    "\n",
    "    if generator_type==\"mlp\":\n",
    "        gan_input = Input(shape=(num_features,))\n",
    "    else:\n",
    "        gan_input = Input(shape=(lstm_batch_size, num_features))\n",
    "    print (gan_input.shape)\n",
    "    x = generator(gan_input)\n",
    "    gan_output= discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss='mean_squared_error', optimizer=opt)\n",
    "#     gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "                          \n",
    "    return gan\n",
    "gan = create_gan(d,on_lstm_model)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def training(epochs=1, batch_size=60, discriminator_type=\"gan\", generator_type=\"lstm\"):\n",
    "\n",
    "    #Loading the data\n",
    "    batch_count = int(train_features.shape[0] / batch_size)\n",
    "    print (\"batch_count\", batch_count)\n",
    "    print (\"batch_size\", batch_size)\n",
    "    print (\"lstm_batch_size\", lstm_batch_size)\n",
    "    \n",
    "    # Creating GAN generator\n",
    "    if generator_type == \"lstm\":\n",
    "        generator = create_lstm_model()\n",
    "    elif generator_type == \"on_lstm\":\n",
    "        generator = create_on_lstm_model()\n",
    "    elif generator_type == \"mlp\":\n",
    "        generator = create_generator()\n",
    "       \n",
    "    # Creating GAN discriminator\n",
    "    if discriminator_type == \"gan\":\n",
    "        discriminator = create_discriminator()\n",
    "    elif discriminator_type == \"wgan\":\n",
    "        discriminator = create_wgan_discriminator()\n",
    "\n",
    "    # Creating GAN \n",
    "    gan = create_gan(discriminator, generator, generator_type)\n",
    "\n",
    "    # Set up label for valid and fake conditions\n",
    "    valid = np.ones((batch_size, 1)) * 1.0 # prevent overconfidence by penalising the discriminator (1=>0.9)\n",
    "    # valid = np.ones((batch_size, 1)) * 0.9 # prevent overconfidence by penalising the discriminator (1=>0.9)\n",
    "    fake = np.zeros((batch_size, 1))    \n",
    "    actual_data_train_all = data_train_scaled[lstm_batch_size:,0]\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(1,epochs+1 ):\n",
    "        for i in range(batch_count):\n",
    "            \n",
    "            # Generate fake time-series from lstm model input\n",
    "            if generator_type == \"mlp\":\n",
    "                noise = np.random.uniform(-1, 1, (batch_size, num_features))\n",
    "                generated_timeseries = generator.predict(noise)               \n",
    "            else:\n",
    "                train_timeseries_batch = train_features[batch_size*i:batch_size*(i+1),:,:]\n",
    "                generated_timeseries = generator.predict(train_timeseries_batch) \n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            actual_data_train = actual_data_train_all[batch_size*i:batch_size*(i+1)]\n",
    "            discriminator.trainable=True\n",
    "            \n",
    "            #Construct different batches of  real and fake data \n",
    "            X = np.concatenate([actual_data_train, generated_timeseries[:,0]])\n",
    "            y = np.concatenate([valid, fake])\n",
    "            d_loss = discriminator.train_on_batch(X, y)\n",
    "            discriminator.trainable=False\n",
    "            \n",
    "            # Train Generator: combined model  (stacked generator and discriminator)\n",
    "            # Train the generator (wants discriminator to mistake images as real)  \n",
    "            if generator_type == \"mlp\":\n",
    "                noise = np.random.uniform(-1, 1, (batch_size, num_features))\n",
    "                g_loss = gan.train_on_batch(noise, valid)\n",
    "            else:\n",
    "                g_loss = gan.train_on_batch(train_timeseries_batch, valid)\n",
    "\n",
    "            # Print out losses\n",
    "            print (\"Epoch: %d, Batch_count: %d, [D loss: %f] [G loss: %f]\" % (epoch, batch_count, d_loss, g_loss))\n",
    "            losses.append([epoch, d_loss, g_loss])\n",
    "        \n",
    "        if epoch == 1 or epoch % 100 == 0:\n",
    "            if generator_type == \"mlp\":\n",
    "                noise_train = np.random.uniform(-1, 1, (train_features.shape[0], num_features))\n",
    "                generated_timeseries_predict_train = generator.predict(noise_train)\n",
    "                noise_test = np.random.uniform(-1, 1, (test_features.shape[0], num_features))\n",
    "                generated_timeseries_predict_test = generator.predict(noise_test)\n",
    "            else:\n",
    "                generated_timeseries_predict_train = generator.predict(train_features)\n",
    "                generated_timeseries_predict_test = generator.predict(test_features)\n",
    "            \n",
    "            # Inverse transform using train fit_transform model for both train and test sets\n",
    "            train_predictions = scaler_train_output.inverse_transform(generated_timeseries_predict_train)\n",
    "            test_predictions = scaler_train_output.inverse_transform(generated_timeseries_predict_test)\n",
    "            \n",
    "            # Plot train and test\n",
    "            plot_timeseries(epoch, \"train\", discriminator_type, generator_type, train_predictions, benchmark_actual_train)\n",
    "            plot_timeseries(epoch, \"test\", discriminator_type, generator_type, test_predictions, benchmark_actual_test)\n",
    "            plot_timeseries2(epoch, \"train\", discriminator_type, generator_type, train_predictions, benchmark_actual_train)\n",
    "            plot_timeseries2(epoch, \"test\", discriminator_type, generator_type, test_predictions, benchmark_actual_test)\n",
    "            \n",
    "            # Save model\n",
    "            generator.save('train_model/generator_model_' + str(discriminator_type) + '_' + str(generator_type) + '_' + str(epoch) + '.h5')\n",
    "            gan.save('train_model/gan_model_' + str(discriminator_type) + '_' + str(generator_type) + '_' + str(epoch) + '.h5')\n",
    "    return test_predictions, losses, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN + ON-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_onlstm_predictions, gan_onlstm_losses, gan_onlstm_generator = training(epochs, batch_size, \"gan\", \"on_lstm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.transpose(gan_onlstm_losses)[0], np.transpose(gan_onlstm_losses)[1], label='d_loss')\n",
    "plt.plot(np.transpose(gan_onlstm_losses)[0], np.transpose(gan_onlstm_losses)[2], label='g_loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_confusion_matrix(selected_features[0], gan_onlstm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_evaluation(method, true, prediction):\n",
    "    # mean absolute error\n",
    "    mae = mean_absolute_error(true, prediction)\n",
    "    # mean absolute percentage error\n",
    "    mape = np.mean(np.abs((true - prediction) / true)) * 100\n",
    "    # root mean squared error\n",
    "    rmse = np.sqrt(mean_squared_error(true, prediction))\n",
    "    # mean squared error\n",
    "    mse = mean_squared_error(true, prediction)\n",
    "    # correlation coefficient\n",
    "    corr = np.corrcoef(np.concatenate((np.transpose(true), np.transpose(prediction)), axis=0))[0,1]\n",
    "    print (\"{}:\\tmae:{:.5}\\tmape:{:.5}\\trmse:{:.5}\\tmse:{:.5}\\tcorr:{:.3}\".format(method, mae, mape, rmse, mse, corr))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_evaluation(\"lstm\\t\", benchmark_actual_test, lstm_predictions)\n",
    "accuracy_evaluation(\"onlstm\\t\", benchmark_actual_test, on_lstm_predictions)\n",
    "accuracy_evaluation(\"gan_onlstm\", benchmark_actual_test, gan_onlstm_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(np.diff(benchmark_actual_test.T[0])).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trading_profit(actual, predict, trading_type):\n",
    "    tc = 0.0015\n",
    "    profit = 0\n",
    "    profit2 = 0\n",
    "    trade_vol = 100000\n",
    "    transaction_count = 0\n",
    "    answer = 0\n",
    "    answer2 = 0\n",
    "    ans_increase = 0\n",
    "    ans_decrease = 0\n",
    "    increase = 0\n",
    "    decrease = 0\n",
    "    for i in range(len(actual)-1):\n",
    "        # print (actual[i], actual[i+1], predict[i], predict[i+1]) \n",
    "        if predict[i] <= predict[i+1]:\n",
    "            trade_diff = actual[i+1] - actual[i]\n",
    "            transaction_count += 1\n",
    "        else:\n",
    "            if trading_type == 'buy_only':\n",
    "                trade_diff = 0\n",
    "            else:\n",
    "                trade_diff = actual[i] - actual[i+1]\n",
    "                transaction_count += 1\n",
    "        answer += abs(actual[i+1] - actual[i]) * trade_vol\n",
    "        answer2 += (abs(actual[i+1] - actual[i])-tc) * trade_vol\n",
    "        if actual[i+1] - actual[i] >=0:\n",
    "            ans_increase += 1\n",
    "            if  predict[i+1] >= predict[i]:\n",
    "                increase += 1\n",
    "        else:\n",
    "            ans_decrease += 1\n",
    "            if  predict[i+1] < predict[i]:\n",
    "                decrease += 1\n",
    "        profit += trade_diff * trade_vol\n",
    "        profit2 += (trade_diff-tc) * trade_vol\n",
    "        #print (i, abs(actual[i+1] - actual[i]))\n",
    "    print (\"\\n[ANSWER] max profit:{:}, {:}\".format(answer, answer2))\n",
    "    print (\"trading_type: {}\\t transaction_count: {}\\t total_profit: {}\\t profit/max_profit: {:.4}\".format(\n",
    "        trading_type, transaction_count, profit, float(profit/answer*100)))\n",
    "    print (\"trading_type2: {}\\t transaction_count: {}\\t total_profit2: {}\\t profit2/max_profit2: {:.4}\".format(\n",
    "        trading_type, transaction_count, profit2, float(profit2/answer2*100)))\n",
    "    print (\"ans_increase:{}\\tans_decrease:{}\\tincrease:{}\\tdecrease:{}\\t{:}\".format(\n",
    "        ans_increase, ans_decrease, increase, decrease, (increase+decrease)/(ans_increase+ans_decrease)*100,))\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_predict1 = trading_profit(benchmark_actual_test, lstm_predictions[:,0], 'buy_only')\n",
    "lstm_predict2 = trading_profit(benchmark_actual_test, lstm_predictions[:,0], 'buy_sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_lstm_predict1 = trading_profit(benchmark_actual_test, on_lstm_predictions[:,0], 'buy_only')\n",
    "on_lstm_predict2 = trading_profit(benchmark_actual_test, on_lstm_predictions[:,0], 'buy_sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_onlstm_predictions1 = trading_profit(benchmark_actual_test, gan_onlstm_predictions, 'buy_only')\n",
    "gan_onlstm_predictions2 = trading_profit(benchmark_actual_test, gan_onlstm_predictions, 'buy_sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
